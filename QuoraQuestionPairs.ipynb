{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuoraQuestionPairs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koralturkk/QuoraQuestionPairs/blob/master/QuoraQuestionPairs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klBJ3JqVS0oe",
        "colab_type": "text"
      },
      "source": [
        "# Quora Question Pairs\n",
        "\n",
        "Quora Question Pairs challenge is a semantic similarity problem. In the notebook, I have followed word and sentence embeddings to vectorize words and sentences to extract patterns and find out degrees of similarity between sentences.\n",
        "\n",
        "The following notebook is consisted of parts listed below to approach the problem. \n",
        "\n",
        "\n",
        "\n",
        "1.   Explanatory Data Analysis\n",
        "\n",
        "\n",
        "          *   Stratified Sampling\n",
        "          *   Train/Val/Test Split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2.   Models for Semantic Similarity\n",
        "\n",
        "\n",
        "          2.1. Sentence Embeddings\n",
        "\n",
        "              *   Google's Universal Sentence Encoder\n",
        "              *   Sentence-BERT \n",
        "\n",
        "\n",
        "          2.2. Word Embeddings\n",
        "\n",
        "              *   Google News's Word2Vec and Siamese Network\n",
        "\n",
        "3. Overview and Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MvFK2Z2URY4",
        "colab_type": "text"
      },
      "source": [
        "In the notebook, pre-trained embedding layers are used to vectorize words and sentences. Transfer Learning in NLP tasks are widely used approach when the computational and data resources are scarce. The embedding models are generated by training process of state of the art models on very large text corpus. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRp6V2Ql8-Ty",
        "colab_type": "text"
      },
      "source": [
        "#### Link to drive and import custom packages\n",
        "\n",
        "The "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHMNGzjj87pM",
        "colab_type": "code",
        "outputId": "eae8d8b0-83c5-4498-9a81-2d50ddf4354a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sDq2ewX9Iu6",
        "colab_type": "text"
      },
      "source": [
        "#### Loading Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7NY6_7ZoxQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow_hub as hub\n",
        "import seaborn as sns\n",
        "import os, re, io, random\n",
        "from absl import logging\n",
        "from google.colab import files\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score \n",
        "\n",
        "#pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adadelta, SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "import time\n",
        "import scipy.spatial\n",
        "import json\n",
        "import torch ## loading file\n",
        "import nltk ## stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense, Input, LSTM, Embedding,Activation,Flatten,merge,TimeDistributed,CuDNNGRU,Bidirectional, GRU,concatenate,subtract,add,maximum,multiply,Layer,Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.backend import backend as K\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import KeyedVectors\n",
        "import itertools\n",
        "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6bgHxBYVd3H",
        "colab_type": "text"
      },
      "source": [
        "\"nltk\" package will provide a list of stop words that will be useful when cleaning the text.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suk9bJ4U2FDh",
        "colab_type": "code",
        "outputId": "c982bdc0-3760-4819-9df9-171bbd2d9357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "stop_words = nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYgV0VpVazxE",
        "colab_type": "text"
      },
      "source": [
        "### Saving and Loading Files\n",
        "\n",
        "Due to the storage heavy nature of embeddings and models, it is a good practice to store processesed embeddings and models to avoid overhead. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A12F0H60aouJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_obj(obj, name):\n",
        "    with open(\"/content/drive/My Drive/Carl_Finance/\"+ name + \".pkl\", \"wb\") as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_obj(name):\n",
        "    with open(\"/content/drive/My Drive/Carl_Finance/\"+ name + \".pkl\", \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "#save = save_obj(embedding_dict, \"embeddings\")\n",
        "\n",
        "#embedding_dict = {}\n",
        "\n",
        "model_save_name = 'embeddings.pt'\n",
        "path = F\"/content/drive/My Drive/Carl_Finance/{model_save_name}\" \n",
        "\n",
        "def add_to_embedding_dict(key:str, value):\n",
        "  embedding_dict.update({key:value})\n",
        "\n",
        "## To save a file\n",
        "#torch.save(embedding_dict, path)\n",
        "#save_obj(embedding_dict,'embeddings')\n",
        "\n",
        "\n",
        "## To load a file \n",
        "#embedding_dict = torch.load(path)\n",
        "\n",
        "embedding_dict = load_obj(\"embeddings\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UWkxMSk-hwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv(\"/content/drive/My Drive/Carl_Finance/train.csv\")\n",
        "#test_data = pd.read_csv(\"/content/drive/My Drive/Carl_Finance/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAYyuerypY-3",
        "colab_type": "text"
      },
      "source": [
        "## Exploratory Data Analysis \n",
        "\n",
        "Make a statement about length of sentences and their contribution to performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9dZj_hepiXA",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing \n",
        "\n",
        "#### Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiRGz3hkrSey",
        "colab_type": "code",
        "outputId": "03cea318-e979-416a-b768-92e3bfb94d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Missing Values in question1 column: {}\".format(train_data[\"question1\"].isnull().sum()))\n",
        "print(\"Missing Values in question2 column: {}\".format(train_data[\"question2\"].isnull().sum()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing Values in question1 column: 1\n",
            "Missing Values in question2 column: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ujPsl2rzJ1",
        "colab_type": "text"
      },
      "source": [
        "#### Removing Rows with Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3kx32YAryA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = train_data.dropna(how='any',axis=0) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSXRC4ENsCf8",
        "colab_type": "code",
        "outputId": "ab7049fa-53b2-4a76-bdc1-c3b90cf2b90e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Missing Values in question1 column: {}\".format(train_data[\"question1\"].isnull().sum()))\n",
        "print(\"Missing Values in question2 column: {}\".format(train_data[\"question2\"].isnull().sum()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing Values in question1 column: 0\n",
            "Missing Values in question2 column: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAtqUxw3rVUe",
        "colab_type": "text"
      },
      "source": [
        "#### Cleaning Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQsyKU6ypolZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_columns(df, columns = [], clean_stop_words = False):\n",
        "\n",
        "  for col in columns:\n",
        "    df.loc[:,col] = df.loc[:,col].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "    df.loc[:,col] = df.loc[:,col].str.replace(r\"\\d+\", \"\")\n",
        "    df.loc[:,col] = df.loc[:,col].str.replace('[^\\w\\s]','')\n",
        "    df.loc[:,col] = df.loc[:,col].str.replace(r\"[︰-＠]\", \"\")\n",
        "\n",
        "    if clean_stop_words:\n",
        "      df.loc[:,col] = df.loc[:,col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words)) ##stop words\n",
        "\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taj2V175TsGH",
        "colab_type": "code",
        "outputId": "2e83a7e3-e85b-47c3-e39c-ccbe1c61ab63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(train_data.tail())\n",
        "print(train_data.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            id  ...  is_duplicate\n",
            "404285  404285  ...             0\n",
            "404286  404286  ...             1\n",
            "404287  404287  ...             0\n",
            "404288  404288  ...             0\n",
            "404289  404289  ...             0\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "   id  qid1  ...                                          question2 is_duplicate\n",
            "0   0     1  ...  What is the step by step guide to invest in sh...            0\n",
            "1   1     3  ...  What would happen if the Indian government sto...            0\n",
            "2   2     5  ...  How can Internet speed be increased by hacking...            0\n",
            "3   3     7  ...  Find the remainder when [math]23^{24}[/math] i...            0\n",
            "4   4     9  ...            Which fish would survive in salt water?            0\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSWfjGLmcPT9",
        "colab_type": "code",
        "outputId": "4401ab6d-bfa9-4be1-a229-1f91636dc6ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "train_data = clean_columns(train_data, [\"question1\", \"question2\"])\n",
        "print(train_data.tail())\n",
        "print(train_data.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            id  ...  is_duplicate\n",
            "404285  404285  ...             0\n",
            "404286  404286  ...             1\n",
            "404287  404287  ...             0\n",
            "404288  404288  ...             0\n",
            "404289  404289  ...             0\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "   id  qid1  ...                                          question2 is_duplicate\n",
            "0   0     1  ...  what is the step by step guide to invest in sh...            0\n",
            "1   1     3  ...  what would happen if the indian government sto...            0\n",
            "2   2     5  ...  how can internet speed be increased by hacking...            0\n",
            "3   3     7  ...    find the remainder when mathmath is divided by             0\n",
            "4   4     9  ...             which fish would survive in salt water            0\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihMx9H2wpxbv",
        "colab_type": "text"
      },
      "source": [
        "### Sampling Training Set\n",
        "\n",
        "Since the data set is too large. I will work on sample data to build NLP model to make to training and optimization process more robust. It is important to preserve the distribution to build a viable model for deployment.\n",
        "\n",
        "Therefore, I will used stratified sampling to sample data that has same distribution of \"is_duplicate\" labels from the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXknxZLlrif1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = train_data[\"is_duplicate\"]\n",
        "X = train_data.copy(deep=False).drop(columns = [\"is_duplicate\"])\n",
        "\n",
        "X_source, X_sample, y_source, y_sample = train_test_split(X, y, test_size=0.1, random_state=42, stratify= y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOm8GkierktI",
        "colab_type": "code",
        "outputId": "97090f3c-77ca-488c-d8e7-19af3f3c1567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y_sample.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    25503\n",
              "1    14926\n",
              "Name: is_duplicate, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fjNvJUwrnXA",
        "colab_type": "code",
        "outputId": "a1839333-52fc-4ead-cc3c-bd47b9867786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Sample distribution of label 1: \",y_sample.value_counts()[1]/y_sample.value_counts().sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample distribution of label 1:  0.369190432610255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldc0BMI2roP4",
        "colab_type": "code",
        "outputId": "a8cbc094-68cb-43a6-8e44-3a99a1f0a6b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_data.is_duplicate.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    255024\n",
              "1    149263\n",
              "Name: is_duplicate, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UBPgpRVrrKO",
        "colab_type": "code",
        "outputId": "acdae457-ced8-4ef2-b708-dbd7a95988f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Source data distribution of label 1: \", train_data.is_duplicate.value_counts()[1]/train_data.is_duplicate.value_counts().sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source data distribution of label 1:  0.3692005926482919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqFLGd_NsIDq",
        "colab_type": "text"
      },
      "source": [
        "The distribution of labels are very close to each other. We can move forward with the implementation on sample data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLemNqxl_omN",
        "colab_type": "text"
      },
      "source": [
        "### Train/Val/Test Set\n",
        "\n",
        "Seperation of data into train/val/test set help us to validate the generalizability of models. \n",
        "\n",
        "The model is trained on training data, tuned on val data and final test is to check results on test data to see how valid the model is on unseen examples. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytzlsGWtAB7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42, stratify = y_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dXb1BGjD0vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.reset_index(inplace=True)\n",
        "X_test.reset_index(inplace=True)\n",
        "#y_train.reset_index(inplace=True)\n",
        "#y_test.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73VoyF89r-aj",
        "colab_type": "text"
      },
      "source": [
        "# Models for Semantic Similarity\n",
        "\n",
        "Models for semantic similarity use embeddings to vectorize sentences or words. Then various distance metrics are calculated to measure variations between vector outputs. Some of the most commonly used distance metrics are Cosine Similarity, Manhattan Distance, Minkowski Distance and Euclidean Distance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_KflXhgqNbd",
        "colab_type": "text"
      },
      "source": [
        "#### Distance Metrics \n",
        "\n",
        "Distance metrics help us identify the relative difference or similarity between different vectors. Which distance metrics to be used relies on the nature of the problem. Moreover, distance metrics that are suitable for the problem scope can be accepted as a hyperparameter, therefore, they can be subjected comparison by the f1 score they yield. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wow17aQ-jHKX",
        "colab_type": "text"
      },
      "source": [
        "#### Useful Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfDVRP65hqSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report_predictions(y_train, predictions):\n",
        "  actual = y_train.to_list()\n",
        "  results = confusion_matrix(actual, predictions) \n",
        "  print('\\nConfusion Matrix :\\n')\n",
        "  print(results) \n",
        "  #print('Accuracy Score :\\n',accuracy_score(actual, predictions))\n",
        "  print('\\nReport : \\n')\n",
        "  print(classification_report(actual, predictions))\n",
        "  #print(\"\\nF1 Score:\\n\")\n",
        "  #print(f1_score(y_train,predictions))\n",
        "\n",
        "def f1_score_and_accuracy(y_train, predictions):\n",
        "  actual = y_train.to_list()\n",
        "  print('Accuracy Score :\\n',accuracy_score(actual, predictions))\n",
        "  print(\"\\nF1 Score:\\n\")\n",
        "  print(f1_score(y_train,predictions))\n",
        "\n",
        "def cosine_similarities(embedding_1, embedding_2):\n",
        "  if len(embedding_1) == len(embedding_2):\n",
        "    length = len(embedding_1)\n",
        "  else:\n",
        "    return print(\"Array sizes do not match\")\n",
        "\n",
        "  similarities = []\n",
        "\n",
        "  for i in range(length):\n",
        "    sentence_1 = np.reshape(embedding_1[i], (1, -1))\n",
        "    sentence_2 = np.reshape(embedding_2[i], (1, -1))\n",
        "    similarity = float(cosine_similarity(sentence_1,sentence_2))\n",
        "    similarities.append(similarity)\n",
        "\n",
        "  return np.reshape(similarities, (-1,1))\n",
        "\n",
        "def get_predictions(similarities, threshold = 0.8):\n",
        "  predictions = list(map(float, similarities>threshold))\n",
        "  return predictions\n",
        "\n",
        "def find_max_word_count(df, columns):\n",
        "  count = 0\n",
        "  for col in columns:\n",
        "    new_count = max(df[col].str.split().map(len))\n",
        "    if new_count > count:\n",
        "      count = new_count\n",
        "\n",
        "    else:\n",
        "      continue\n",
        "  return count\n",
        "\n",
        "\n",
        "def tokenize(df, questions_cols):\n",
        " \n",
        "  for index, row in df.iterrows():\n",
        "\n",
        "      for question in questions_cols:\n",
        "\n",
        "          q2n = [] \n",
        "          for word in row[question].split():\n",
        "              if word in stop_words and word not in word2vec.vocab:\n",
        "                  continue\n",
        "\n",
        "              if word not in vocabulary:\n",
        "                  vocabulary[word] = len(inverse_vocabulary)\n",
        "                  q2n.append(len(inverse_vocabulary))\n",
        "                  inverse_vocabulary.append(word)\n",
        "              else:\n",
        "                  q2n.append(vocabulary[word])\n",
        "\n",
        "          df.set_value(index, question, q2n)\n",
        "\n",
        "  return df\n",
        "\n",
        "def build_embedding_matrix(vocabulary, embedding_dim =300):\n",
        "  embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "  embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "# Build the embedding matrix\n",
        "  for word, index in vocabulary.items():\n",
        "    if word in word2vec.vocab:\n",
        "      embeddings[index] = word2vec.word_vec(word)\n",
        "\n",
        "  return embeddings "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9sM0aBngpOX",
        "colab_type": "text"
      },
      "source": [
        "## Sentence Embedding Models\n",
        "\n",
        "Sentence Embedding Models embeds full sentences into a vector representation. On this task I will implement and assess results of Universal Sentence Encoder and Sentence-Bert.\n",
        "\n",
        "\n",
        "*   Google's Universal Sentence Encoder\n",
        "\n",
        "        1.   Deep Averaging Network\n",
        "        2.   Transformer\n",
        "\n",
        "\n",
        "*   Sentence-Bert \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMwLlbYFsDwY",
        "colab_type": "text"
      },
      "source": [
        "Paper \n",
        "Take a look!!   \n",
        "-https://arxiv.org/pdf/1907.04307.pdf  \n",
        "-https://www.learnopencv.com/universal-sentence-encoder/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yePjxdLp76Yz",
        "colab_type": "text"
      },
      "source": [
        "### Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEZX0aKO_dzJ",
        "colab_type": "text"
      },
      "source": [
        "There are 2 embeddings shared with the community in Universal Sentence Encoder. Both trained with different architectures, Deep Averaging Networks and Transformer Networks. These architectures present trade-offs, DAN model is computationally less expensive and has less accuracy overall while the model with transformer encoder scores higher accuracy with more computational costs. \n",
        "\n",
        "I will try to implement both for the project to see their impact on predictions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV8ewnAvAF8m",
        "colab_type": "text"
      },
      "source": [
        "#### Universal Sentence Encoder Trained with Deep Averaging Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ5qUC3J74Wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DAN_module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\"]\n",
        "\n",
        "# Reduce logging output.\n",
        "#logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "#DAN_module_embedding = hub.KerasLayer(DAN_module_url)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmMmDdmB-3KB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Create\n",
        "\n",
        "#question_1_list = X_train.loc[:,\"question1\"].to_list()\n",
        "#question_2_list = X_train.loc[:,\"question2\"].to_list()\n",
        "\n",
        "\n",
        "#with tf.Session() as session:\n",
        " # session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  #question_1_DAN_embeddings = session.run(DAN_module_embedding(question_1_list))\n",
        "  #question_2_DAN_embeddings = session.run(DAN_module_embedding(question_2_list))\n",
        "\n",
        "## Load\n",
        "question_1_DAN_embeddings, question_2_DAN_embeddings = embedding_dict[\"question_1_DAN_embeddings\"], embedding_dict[\"question_2_DAN_embeddings\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfweCVkX_TQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DAN_similarities = cosine_similarities(question_1_DAN_embeddings,question_2_DAN_embeddings)\n",
        "DAN_predictions = get_predictions(DAN_similarities, 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbRNRnbKU5OX",
        "colab_type": "code",
        "outputId": "125fb857-b334-4e66-801b-74e8b3f16bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "DAN_report = report_predictions(y_train, DAN_predictions)\n",
        "DAN_report"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix :\n",
            "[[15809  4593]\n",
            " [ 4214  7727]]\n",
            "Accuracy Score : 0.7276999659895496\n",
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78     20402\n",
            "           1       0.63      0.65      0.64     11941\n",
            "\n",
            "    accuracy                           0.73     32343\n",
            "   macro avg       0.71      0.71      0.71     32343\n",
            "weighted avg       0.73      0.73      0.73     32343\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSw0620apHkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Save\n",
        "#add_to_embedding_dict(\"question_1_DAN_embeddings\",question_1_DAN_embeddings)\n",
        "#add_to_embedding_dict(\"question_2_DAN_embeddings\",question_2_DAN_embeddings)\n",
        "#torch.save(embedding_dict, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgFFGJZPCCTH",
        "colab_type": "text"
      },
      "source": [
        "#### Universal Sentence Encoder with Transformer Encoder\n",
        "\n",
        "Crashed due to RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMgWjBbDCGOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TRANS_module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "#TRANS_module_embedding = hub.load(TRANS_module_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUO2aBOCDmHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with tf.Session() as session:\n",
        " # session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  #question_1_TRANS_embeddings, question_2_TRANS_embeddings = session.run([TRANS_module_embedding(question_1_list), TRANS_module_embedding(question_2_list)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whXW606lEAbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TRANS_similarities = cosine_similarities(question_1_TRANS_embeddings,question_2_TRANS_embeddings)\n",
        "#TRANS_predictions = get_predictions(similarities, 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucy1dUjaEL2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#report_predictions(y_train, TRANS_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cHec7GkWmb5",
        "colab_type": "text"
      },
      "source": [
        "### Sentence-BERT\n",
        "\n",
        "\n",
        "Researchers in Ubiquitous Knowledge Processing Labb (UKP-TUDA) implemented Sentence-Bert model which is a modification of a pretrained BERT network. The model uses siamese and triplet network to derive semantically meaningful sentence embeddings. This way, they have utilized BERT to be used for new tasks such as semantic similarity which was not possible before.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N9WoMf0fIN8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Encoding Text into Sentence Embeddings\n",
        "\n",
        "#bert_nli_stsb_mean_transformer = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n",
        "#bert_nli_stsb_mean_question_1_embeddings = bert_nli_stsb_mean_transformer.encode(question_1_list)\n",
        "#bert_nli_stsb_mean_question_2_embeddings = bert_nli_stsb_mean_transformer.encode(question_2_list)\n",
        "\n",
        "### Loading pre-encoded sentence embeddings\n",
        "\n",
        "bert_nli_stsb_mean_question_1_embeddings = embedding_dict[\"bert_nli_stsb_mean_question_1_embeddings\"]\n",
        "bert_nli_stsb_mean_question_2_embeddings = embedding_dict[\"bert_nli_stsb_mean_question_2_embeddings\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKg4FlK-XYID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## To save a file\n",
        "\n",
        "#add_to_embedding_dict(\"bert_nli_stsb_mean_question_1_embeddings\",bert_nli_stsb_mean_question_1_embeddings)\n",
        "#add_to_embedding_dict(\"bert_nli_stsb_mean_question_2_embeddings\", bert_nli_stsb_mean_question_2_embeddings)\n",
        "\n",
        "#save_obj(embedding_dict,\"embeddings\")\n",
        "\n",
        "#torch.save(embedding_dict, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN0Vu1F3-UMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sbert_similarities = cosine_similarities(bert_nli_stsb_mean_question_1_embeddings,bert_nli_stsb_mean_question_2_embeddings)\n",
        "sbert_predictions = get_predictions(sbert_similarities, 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKYm2tnp-Xz6",
        "colab_type": "code",
        "outputId": "f6c6c633-f302-4cea-a0f4-b5a44845f2e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "sbert_report = report_predictions(y_train, sbert_predictions)\n",
        "sbert_report"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix :\n",
            "[[16234  4168]\n",
            " [ 4578  7363]]\n",
            "Accuracy Score : 0.7295860000618372\n",
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.80      0.79     20402\n",
            "           1       0.64      0.62      0.63     11941\n",
            "\n",
            "    accuracy                           0.73     32343\n",
            "   macro avg       0.71      0.71      0.71     32343\n",
            "weighted avg       0.73      0.73      0.73     32343\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAt4djr34mKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#del bert_nli_stsb_mean_transformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P1avGVWjhe7",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding Models\n",
        "\n",
        "In the word embedding models, in order to have achieve a \"is duplicate\" binary classification objective, we need to feed word embedding sequences to a siamese network. Then, the encoded sequence will decide how much semantically similar these two input sequences by calculating the distance function using the two outputs of the network. \n",
        "\n",
        "The choice of word embedding to vectorize words is important for performance. Therefore, I would choose to work with Google News' word2vec embeddings. \n",
        "\n",
        "The selection of word embeddings has to be done empirically. Each trial has to be documented and we should deploy model that yields the highest accuracy and f1 score. \n",
        "\n",
        "\n",
        "In this notebook, I will implement Siamese Manhattan LSTM. A model that yields high accuracy in the task to compare the word embedding result to sentence embedding results of state of the art models. \n",
        "\n",
        "Later, I generate a wide hyperparameter space and by using Gaussian optimization, I will try to come up with a custom architecture to compete against the preexisting models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03bKEV9_qsM7",
        "colab_type": "code",
        "outputId": "0264f621-043d-4fa8-c771-49edf20748ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## max word count is needed to implement sequences. So that we can instantiate sequences that can encapsulate all the sentence in the dataset. \n",
        "max_word_count = find_max_word_count(train_data,[\"question1\", \"question2\"])\n",
        "print(max_word_count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax4tfhrYL3tP",
        "colab_type": "text"
      },
      "source": [
        "#### Word2Vec\n",
        "\n",
        "After the creation of embedding matrix for the words that we have, it is not necessary to load the GoogleNews Embedding File due to its size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8xTDcx9L5U6",
        "colab_type": "code",
        "outputId": "3e9bc964-215e-45e0-f238-9818f8871c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#word2vec_embedding_file = \"/content/drive/My Drive/Carl_Finance/GoogleNews-vectors-negative300.bin.gz\"\n",
        "#word2vec = KeyedVectors.load_word2vec_format(word2vec_embedding_file, binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNvwehwGYbwv",
        "colab_type": "text"
      },
      "source": [
        "#### Word Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzTY54JZuzpZ",
        "colab_type": "code",
        "outputId": "5f499eeb-f7d8-480d-9c01-b36f7b111261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
        "questions_cols = ['question1', 'question2']\n",
        "\n",
        "## Tokenize questions \n",
        "\n",
        "X_w2v = clean_columns(X_sample.copy(deep=False), columns = questions_cols, clean_stop_words = True)\n",
        "X_w2v = tokenize(X_w2v,questions_cols)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUetaY3ZMVzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Embedding \n",
        "word_2_vec_embeddings = embedding_dict[\"word_2_vec_embeddings\"]   #build_embedding_matrix(vocabulary,embedding_dim =300) \n",
        "\n",
        "## Delete \n",
        "#del word2vec\n",
        "\n",
        "#add_to_embedding_dict(\"word_2_vec_embeddings\",word_2_vec_embeddings)\n",
        "\n",
        "#save_obj(embedding_dict,\"embeddings\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIf5Mfx2NCa-",
        "colab_type": "code",
        "outputId": "25c08184-d63c-4b9d-c5da-ff6a8bbe1e48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "X_w2v.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>156326</th>\n",
              "      <td>156326</td>\n",
              "      <td>244682</td>\n",
              "      <td>244683</td>\n",
              "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]</td>\n",
              "      <td>[12, 1, 2, 13, 9, 14, 15, 10, 16, 17, 18, 19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233222</th>\n",
              "      <td>233222</td>\n",
              "      <td>343427</td>\n",
              "      <td>343428</td>\n",
              "      <td>[20, 21, 22, 23, 24]</td>\n",
              "      <td>[25, 21, 26, 27, 28]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318647</th>\n",
              "      <td>318647</td>\n",
              "      <td>444025</td>\n",
              "      <td>444026</td>\n",
              "      <td>[29, 30, 31]</td>\n",
              "      <td>[29, 30, 31]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401836</th>\n",
              "      <td>401836</td>\n",
              "      <td>535283</td>\n",
              "      <td>535284</td>\n",
              "      <td>[32, 33, 34]</td>\n",
              "      <td>[32, 33]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3053</th>\n",
              "      <td>3053</td>\n",
              "      <td>6053</td>\n",
              "      <td>6054</td>\n",
              "      <td>[35, 36]</td>\n",
              "      <td>[35, 36]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  ...                                      question2\n",
              "156326  156326  ...  [12, 1, 2, 13, 9, 14, 15, 10, 16, 17, 18, 19]\n",
              "233222  233222  ...                           [25, 21, 26, 27, 28]\n",
              "318647  318647  ...                                   [29, 30, 31]\n",
              "401836  401836  ...                                       [32, 33]\n",
              "3053      3053  ...                                       [35, 36]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBT_czm4enPV",
        "colab_type": "text"
      },
      "source": [
        "### Manhattan LSTM \n",
        "\n",
        "The manhattan distance is used in the model. In the article, it is said that manhattan distance outperforms the cosine similarity. So I will follow the same approach during the implementation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLlTblATTeUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_w2v, X_val_w2v, y_train_w2v, y_val_w2v = train_test_split(X_w2v, y_sample, test_size=0.2, random_state=42, stratify = y_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVH-EAOSbg3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Changing X_train_w2v to dict will provide greater ease at siamese network\n",
        "\n",
        "X_train_w2v = {'left': X_train_w2v.question1, 'right': X_train_w2v.question2}\n",
        "X_val_w2v = {'left': X_val_w2v.question1, 'right': X_val_w2v.question2}\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "y_train_w2v = y_train_w2v.values\n",
        "y_val_w2v = y_val_w2v.values\n",
        "\n",
        "# Zero padding\n",
        "for dataset, side in itertools.product([X_train_w2v, X_val_w2v], ['left', 'right']):\n",
        "    dataset[side] = pad_sequences(dataset[side], maxlen=max_word_count)\n",
        "\n",
        "# Make sure everything is ok\n",
        "assert X_train_w2v['left'].shape == X_train_w2v['right'].shape\n",
        "assert len(X_train_w2v['left']) == len(y_train_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjB9zoT_eXpN",
        "colab_type": "code",
        "outputId": "7ed28d99-5763-48b8-ed3c-9896dd4ee9e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "### Model Implementation\n",
        "\n",
        "# Model variables\n",
        "embedding_dim = 300\n",
        "n_hidden = 50\n",
        "gradient_clipping_norm = 1.25\n",
        "batch_size = 64\n",
        "n_epoch = 25\n",
        "\n",
        "\n",
        "def cosine_distance(x1, x2):\n",
        "    x1 = K.l2_normalize(x1, axis=-1)\n",
        "    x2 = K.l2_normalize(x2, axis=-1)\n",
        "    return -K.mean(x1 * x2, axis=-1, keepdims=True)\n",
        "\n",
        "def cos_dist_output_shape(shape_1, shape_2):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0],1)\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "\n",
        "# The visible layer\n",
        "def create_model():\n",
        "  left_input = Input(shape=(max_word_count,), dtype='int32')\n",
        "  right_input = Input(shape=(max_word_count,), dtype='int32')\n",
        "\n",
        "  embedding_layer = Embedding(len(word_2_vec_embeddings), embedding_dim, weights=[word_2_vec_embeddings], input_length=max_word_count, trainable=False)\n",
        "\n",
        "  # Embedded version of the inputs\n",
        "  encoded_left = embedding_layer(left_input)\n",
        "  encoded_right = embedding_layer(right_input)\n",
        "\n",
        "  # Since this is a siamese network, both sides share the same LSTM\n",
        "  shared_lstm = LSTM(n_hidden)\n",
        "\n",
        "  left_output = shared_lstm(encoded_left)\n",
        "  right_output = shared_lstm(encoded_right)\n",
        "\n",
        "\n",
        "  # Calculates the distance as defined by the MaLSTM model\n",
        "  malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "  # Pack it all up into a model\n",
        "  malstm = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "  # Adadelta optimizer, with gradient clipping by norm\n",
        "  optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "\n",
        "\n",
        "  ## Callbacks\n",
        "\n",
        "  ## Learning Rate \n",
        "\n",
        "  #lr_schedule = LearningRateScheduler(lambda epoch: 1e-8*10**(epoch/5))\n",
        "\n",
        "  ## Checkpoint Callback\n",
        "  filepath=\"/content/drive/My Drive/Carl_Finance/weights.best.hdf5\"\n",
        "  model_checkpoint = ModelCheckpoint(filepath, monitor=\"val_acc\", verbose=1, save_best_only=True, mode='max')\n",
        "  callbacks_list = [model_checkpoint]\n",
        "\n",
        "  # load weights\n",
        "\n",
        "  malstm.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "  # Start training\n",
        "\n",
        "  #malstm = malstm.fit([X_train_w2v['left'], X_train_w2v['right']], y_train_w2v, batch_size=batch_size, epochs=n_epoch,\n",
        "  #                           validation_data=([X_val_w2v['left'], X_val_w2v['right']], y_val_w2v),callbacks=callbacks_list, verbose = 1)\n",
        "\n",
        "  return malstm\n",
        "\n",
        "malstm = create_model() \n",
        "malstm.load_weights(\"/content/drive/My Drive/Carl_Finance/weights.best.hdf5\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcKumaSYjsCy",
        "colab_type": "code",
        "outputId": "9e2e30c0-fb61-4b48-e24e-11d96b012d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "malstm.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 237)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 237)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 237, 300)     9497700     input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 50)           70200       embedding_1[0][0]                \n",
            "                                                                 embedding_1[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 1)            0           lstm_1[0][0]                     \n",
            "                                                                 lstm_1[1][0]                     \n",
            "==================================================================================================\n",
            "Total params: 9,567,900\n",
            "Trainable params: 70,200\n",
            "Non-trainable params: 9,497,700\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkRnIMmnQoMH",
        "colab_type": "code",
        "outputId": "833baa6a-477b-4aee-ea3e-921457375a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "malstm_pred = malstm.predict([X_val_w2v['left'], X_val_w2v['right']])\n",
        "malstm_report = report_predictions(y_val_w2v,get_predictions(malstm_pred))\n",
        "malstm_report"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix :\n",
            "[[990  30]\n",
            " [499  98]]\n",
            "Accuracy Score : 0.6728509585652442\n",
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.97      0.79      1020\n",
            "           1       0.77      0.16      0.27       597\n",
            "\n",
            "    accuracy                           0.67      1617\n",
            "   macro avg       0.72      0.57      0.53      1617\n",
            "weighted avg       0.70      0.67      0.60      1617\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKyEr9inCrPx",
        "colab_type": "text"
      },
      "source": [
        "## Custom Architecture with Hyperparameter Tuning using Bayesian Optimization\n",
        "\n",
        "Sequential model-based optimization\n",
        "\n",
        "Sequential model-based optimization is a Bayesian optimization technique that uses information from past trials to inform the next set of hyperparameters to explore, and there are two variants of this algorithm used in practice:one based on the Gaussian process and the other on the Tree Parzen Estimator. The HyperOpt package implements the Tree Parzen Estimator algorithm to perform optimization. The Tree Parzen Estimator replaces the generative process of choosing parameters from the search space in a tree like fashion with a set of non parametric distributions. \n",
        "\n",
        "- https://blog.dominodatalab.com/hyperopt-bayesian-hyperparameter-optimization/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCPZIj9epf6x",
        "colab_type": "text"
      },
      "source": [
        "#### Sampling Again\n",
        "\n",
        "\n",
        "We need to sample the dataset again due to the computational overhead. It is very time heavy process to search parameter space. Therefore I would like to decrease the number of training examples. The parameters given out from the sample may not reflect fully the optimal parameters for the whole dataset. However, they may present a close picture to what parameters should be. Given the resources I have, it seemed to be the best approach. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAGso2L7SRB_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_sample_w2v, X_source_w2v, y_sample_w2v, y_source_w2v = train_test_split(X_w2v, y_sample, test_size=0.8, random_state=42, stratify = y_sample)\n",
        "X_train_bay, X_val_bay, y_train_bay, y_val_bay = train_test_split(X_sample_w2v, y_sample_w2v, test_size=0.2, random_state=42, stratify = y_sample_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdI0x_i2H13N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Changing X_train_w2v to dict will provide greater ease at siamese network\n",
        "\n",
        "X_train_bay = {'left': X_train_bay.question1, 'right': X_train_bay.question2}\n",
        "X_val_bay = {'left': X_val_bay.question1, 'right': X_val_bay.question2}\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "y_train_bay = y_train_bay.values\n",
        "y_val_bay = y_val_bay.values\n",
        "\n",
        "# Zero padding\n",
        "for dataset, side in itertools.product([X_train_bay, X_val_bay], ['left', 'right']):\n",
        "    dataset[side] = pad_sequences(dataset[side], maxlen=max_word_count)\n",
        "\n",
        "# Make sure everything is ok\n",
        "assert X_train_bay['left'].shape == X_train_bay['right'].shape\n",
        "assert len(X_train_bay['left']) == len(y_train_bay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vj7wJDFNfpC",
        "colab_type": "code",
        "outputId": "7f3d70aa-4f69-4f3f-aca9-9f3fb1887042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
        "from keras import optimizers\n",
        "from keras.layers import GRU\n",
        "\n",
        "bayesian_history= pd.DataFrame(columns=[\"model\",\"loss\", \"acc\", \"val_acc\",\"batch_size\",\"nodes\",\"learning_rate\",\"arch\",\"epochs\",\"optimizer\",\"distance_metric\"])\n",
        "\n",
        "search_space = {\n",
        "    \"batch_size\": hp.choice(\"batch_size\", [8,16,32,64,128]),\n",
        "    \"nodes\": hp.choice(\"nodes\", [25,50,75,100]),\n",
        "    \"learning_rate\":  hp.loguniform(\"learning_rate\", np.log(0.01), np.log(0.2)),\n",
        "    \"arch\": hp.choice(\"arch\",[\"bidirectional\", \"unidirectional\",\"GRU\"]),\n",
        "    \"epochs\": hp.choice(\"epochs\", [15, 20, 25]),\n",
        "    \"optimizer\": hp.choice(\"optimizer\",[\"sgd\", \"rms\",\"adamdelta\"]),\n",
        "    \"distance_metric\": hp.choice(\"distance_metric\",[\"exponent_neg_manhattan_distance\", \"cosine_distance\"]),\n",
        "}\n",
        "\n",
        "\n",
        "def build_siamese_network(params):\n",
        "\n",
        "\n",
        "  left_input = Input(shape=(max_word_count,), dtype='int32')\n",
        "  right_input = Input(shape=(max_word_count,), dtype='int32')\n",
        "\n",
        "  embedding_layer = Embedding(len(word_2_vec_embeddings), embedding_dim, weights=[word_2_vec_embeddings], input_length=max_word_count, trainable=False)\n",
        "\n",
        "  encoded_left = embedding_layer(left_input)\n",
        "  encoded_right = embedding_layer(right_input)\n",
        "\n",
        "  if params[\"arch\"] == \"bidirectional\":\n",
        "    model = Bidirectional(LSTM(params[\"nodes\"])) \n",
        "  elif params[\"arch\"] == \"GRU\":\n",
        "    model = GRU(params[\"nodes\"])\n",
        "  else:\n",
        "    model = LSTM(params[\"nodes\"]) \n",
        "\n",
        "  left_output = model(encoded_left)\n",
        "  right_output = model(encoded_right)\n",
        "\n",
        "\n",
        "  if params[\"distance_metric\"] == \"exponent_neg_manhattan_distance\":\n",
        "    distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "  if params[\"distance_metric\"] == \"cosine_distance\":\n",
        "    distance = Lambda(function=lambda x: cosine_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "  model = Model([left_input, right_input], [distance])\n",
        "\n",
        "  ### Calbacks \n",
        "  file_name = 'siamese_callbacks.csv'\n",
        "  callback_file = F\"/content/drive/My Drive/Carl_Finance/{file_name}\" \n",
        "  csv_callback_bay = keras.callbacks.CSVLogger(callback_file, separator=',', append=False)\n",
        "\n",
        "  early_stop_bay = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=3)\n",
        "  \n",
        "  filepath_bay =\"/content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\"\n",
        "  checkpoint_bay = ModelCheckpoint(filepath_bay, monitor=\"val_acc\", verbose=1, save_best_only=True, mode='max')\n",
        "  \n",
        "  callbacks_list_bay = [early_stop_bay,checkpoint_bay,csv_callback_bay]\n",
        "\n",
        "  lr = params[\"learning_rate\"]\n",
        "  epochs = params[\"epochs\"]\n",
        "  batch_size = params[\"batch_size\"]\n",
        "\n",
        "  if params[\"optimizer\"] == 'rms':\n",
        "      optimizer = optimizers.RMSprop(lr=lr)\n",
        "  elif params[\"optimizer\"] == \"adadelta\":\n",
        "      optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "  else:\n",
        "      optimizer = optimizers.SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "  model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  history = model.fit([X_train_bay['left'], X_train_bay['right']], y_train_bay, batch_size=batch_size, epochs=epochs,\n",
        "                            validation_data=([X_val_bay['left'], X_val_bay['right']], y_val_bay),verbose=2,callbacks=callbacks_list_bay)\n",
        "  \n",
        "  val_error = np.amin(history.history['val_loss']) \n",
        "\n",
        "\n",
        "  ## Record Keeping for Parameter Space\n",
        "  print('Best validation error of epoch:', val_error)\n",
        "\n",
        "  record= pd.DataFrame({\"model\": [model],\"loss\": [np.amin(history.history['loss'])],\n",
        "                        \"val_error\":[np.amin(history.history['val_loss'])],\n",
        "                        \"acc\":[np.amin(history.history['acc'])],\n",
        "                        \"val_acc\":[np.amin(history.history['val_acc'])],\n",
        "                        \"batch_size\":[params[\"batch_size\"]],\"nodes\":[params[\"nodes\"]],\n",
        "                        \"learning_rate\":[params[\"learning_rate\"]],\"arch\":[params[\"arch\"]],\n",
        "                        \"epochs\":[params[\"epochs\"]],\"optimizer\":[params[\"optimizer\"]],\n",
        "                        \"distance_metric\":[params[\"distance_metric\"]]})\n",
        "  \n",
        "  bayesian_history.append(record, ignore_index = True)\n",
        "  bayesian_history.to_csv('/content/drive/My Drive/Carl_Finance/bayesian_history.csv')\n",
        "\n",
        "  ## Save DataFrame\n",
        "  save_obj(bayesian_history,\"bayesian_history_dataframe.pt\")\n",
        "\n",
        "  return {'loss': val_error, 'status': STATUS_OK, 'model': model, \"history\":history}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(build_siamese_network,\n",
        "    space=search_space,\n",
        "    algo=tpe.suggest, # type random.suggest to select param values randomly\n",
        "    max_evals=5, # max number of evaluations you want to do on objective function\n",
        "    trials=trials)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6468 samples, validate on 1617 samples\n",
            "Epoch 1/25\n",
            " - 177s - loss: 0.2764 - acc: 0.6572 - val_loss: 0.2497 - val_acc: 0.6685\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.66852, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 2/25\n",
            " - 166s - loss: 0.2225 - acc: 0.6948 - val_loss: 0.2178 - val_acc: 0.6889\n",
            "\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.66852 to 0.68893, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 3/25\n",
            " - 164s - loss: 0.1951 - acc: 0.7217 - val_loss: 0.2079 - val_acc: 0.6970\n",
            "\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.68893 to 0.69697, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 4/25\n",
            " - 164s - loss: 0.1816 - acc: 0.7348 - val_loss: 0.2041 - val_acc: 0.6988\n",
            "\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.69697 to 0.69882, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 5/25\n",
            " - 161s - loss: 0.1731 - acc: 0.7498 - val_loss: 0.2015 - val_acc: 0.7019\n",
            "\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.69882 to 0.70192, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 6/25\n",
            " - 162s - loss: 0.1668 - acc: 0.7588 - val_loss: 0.1997 - val_acc: 0.7087\n",
            "\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.70192 to 0.70872, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 7/25\n",
            " - 163s - loss: 0.1596 - acc: 0.7743 - val_loss: 0.1984 - val_acc: 0.7069\n",
            "\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.70872\n",
            "Epoch 8/25\n",
            " - 164s - loss: 0.1564 - acc: 0.7788 - val_loss: 0.1979 - val_acc: 0.7044\n",
            "\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.70872\n",
            "Epoch 9/25\n",
            " - 163s - loss: 0.1525 - acc: 0.7907 - val_loss: 0.1971 - val_acc: 0.7044\n",
            "\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.70872\n",
            "Best validation error of epoch:\n",
            "0.19708634840166708\n",
            " 20%|██        | 1/5 [25:07<1:40:29, 1507.32s/it, best loss: 0.19708634840166708]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:7138: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
            "of pandas will change to not sort by default.\n",
            "\n",
            "To accept the future behavior, pass 'sort=False'.\n",
            "\n",
            "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
            "\n",
            "  sort=sort,\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 6468 samples, validate on 1617 samples\n",
            "Epoch 1/25\n",
            " - 43s - loss: 0.3803 - acc: 0.6308 - val_loss: 0.3802 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.63080, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 2/25\n",
            " - 34s - loss: 0.3800 - acc: 0.6308 - val_loss: 0.3799 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.63080\n",
            "Epoch 3/25\n",
            " - 34s - loss: 0.3799 - acc: 0.6308 - val_loss: 0.3798 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.63080\n",
            "Epoch 4/25\n",
            " - 33s - loss: 0.3798 - acc: 0.6308 - val_loss: 0.3798 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.63080\n",
            "Best validation error of epoch:\n",
            "0.3797537739179572\n",
            "Train on 6468 samples, validate on 1617 samples\n",
            "Epoch 1/25\n",
            " - 51s - loss: 0.2047 - acc: 0.6916 - val_loss: 0.2018 - val_acc: 0.6914\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.69140, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 2/25\n",
            " - 42s - loss: 0.1979 - acc: 0.7039 - val_loss: 0.2001 - val_acc: 0.7001\n",
            "\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.69140 to 0.70006, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 3/25\n",
            " - 42s - loss: 0.1921 - acc: 0.7130 - val_loss: 0.1986 - val_acc: 0.7001\n",
            "\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.70006\n",
            "Epoch 4/25\n",
            " - 42s - loss: 0.1872 - acc: 0.7220 - val_loss: 0.1978 - val_acc: 0.7032\n",
            "\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.70006 to 0.70315, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 5/25\n",
            " - 42s - loss: 0.1831 - acc: 0.7291 - val_loss: 0.1969 - val_acc: 0.7075\n",
            "\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.70315 to 0.70748, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 6/25\n",
            " - 42s - loss: 0.1793 - acc: 0.7347 - val_loss: 0.1962 - val_acc: 0.7075\n",
            "\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.70748\n",
            "Epoch 7/25\n",
            " - 42s - loss: 0.1762 - acc: 0.7418 - val_loss: 0.1955 - val_acc: 0.7075\n",
            "\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.70748\n",
            "Epoch 8/25\n",
            " - 42s - loss: 0.1733 - acc: 0.7463 - val_loss: 0.1949 - val_acc: 0.7093\n",
            "\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.70748 to 0.70934, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 9/25\n",
            " - 42s - loss: 0.1705 - acc: 0.7519 - val_loss: 0.1942 - val_acc: 0.7062\n",
            "\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.70934\n",
            "Epoch 10/25\n",
            " - 42s - loss: 0.1680 - acc: 0.7566 - val_loss: 0.1936 - val_acc: 0.7044\n",
            "\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.70934\n",
            "Epoch 11/25\n",
            " - 42s - loss: 0.1658 - acc: 0.7599 - val_loss: 0.1929 - val_acc: 0.7087\n",
            "\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.70934\n",
            "Best validation error of epoch:\n",
            "0.19290456345365897\n",
            "Train on 6468 samples, validate on 1617 samples\n",
            "Epoch 1/20\n",
            " - 345s - loss: 0.3735 - acc: 0.6308 - val_loss: 0.3717 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.63080, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 2/20\n",
            " - 335s - loss: 0.3715 - acc: 0.6308 - val_loss: 0.3713 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.63080\n",
            "Epoch 3/20\n",
            " - 333s - loss: 0.3707 - acc: 0.6308 - val_loss: 0.3706 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.63080\n",
            "Epoch 4/20\n",
            " - 332s - loss: 0.3708 - acc: 0.6308 - val_loss: 0.3712 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.63080\n",
            "Best validation error of epoch:\n",
            "0.37064151763547404\n",
            "Train on 6468 samples, validate on 1617 samples\n",
            "Epoch 1/20\n",
            " - 658s - loss: 0.3908 - acc: 0.6308 - val_loss: 0.3901 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.63080, saving model to /content/drive/My Drive/Carl_Finance/bayesian_weights_best.hdf5\n",
            "Epoch 2/20\n",
            " - 646s - loss: 0.3882 - acc: 0.6308 - val_loss: 0.3887 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.63080\n",
            "Epoch 3/20\n",
            " - 641s - loss: 0.3864 - acc: 0.6308 - val_loss: 0.3875 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.63080\n",
            "Epoch 4/20\n",
            " - 640s - loss: 0.3848 - acc: 0.6308 - val_loss: 0.3864 - val_acc: 0.6308\n",
            "\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.63080\n",
            "Best validation error of epoch:\n",
            "0.38635387587382347\n",
            "100%|██████████| 5/5 [1:42:08<00:00, 1518.83s/it, best loss: 0.19290456345365897]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c20pf2hplUG6",
        "colab_type": "code",
        "outputId": "106b875a-2c9f-4992-dbb9-12f156b3ffe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(trials.best_trial['result']['loss'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<keras.callbacks.History object at 0x7f4fa335bf28>\n",
            "0.19290456345365897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHIhXih-LU3f",
        "colab_type": "text"
      },
      "source": [
        "## Model Comparison \n",
        "\n",
        "Since there is not intensive class imbalance. I will use accuracy metric to compare models. I choosed to obtain accuracy metrics using validation sets since there is no hyperparameter tuning done on the models. Therefore, these data has not been touched in a way that harms the generalizability of the results. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzwc76_bbIKS",
        "colab_type": "code",
        "outputId": "5e84b226-bcdd-4275-a8ee-0fcd2694b092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "print(\"\\n--- Universal Sentence Encoder by DAN Accuracy ---\\n\")\n",
        "print(accuracy_score(y_train, DAN_predictions))\n",
        "\n",
        "print(\"\\n--- S-BERT Accuracy ---\\n\")\n",
        "print(accuracy_score(y_train, sbert_predictions))\n",
        "\n",
        "print(\"\\n--- MALSTM Accuracy ---\\n\")\n",
        "print(accuracy_score(y_val_w2v, get_predictions(malstm_pred)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "--- Universal Sentence Encoder by DAN Accuracy ---\n",
            "\n",
            "0.7276999659895496\n",
            "\n",
            "--- S-BERT Accuracy ---\n",
            "\n",
            "0.7295860000618372\n",
            "\n",
            "--- MALSTM Accuracy ---\n",
            "\n",
            "0.6728509585652442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK6x4W-d1u7e",
        "colab_type": "text"
      },
      "source": [
        "### Takeaways\n",
        "\n",
        "In NLP tasks, the first challenge is always to turn text into numerical representation. Researchers have tried different methodologies to achieve this task over the years. Until 2016 Bag of Words and TF-IDF were popular approaches to achieve high performances. These models rely on frequency that given words appear on the corpus and featurize words based on their number of occurence in sequences. \n",
        "\n",
        "After 2016, embeddings became popular in the NLP community. Embeddings differ from BOW and TF-IDF due to the fact that they try to represent the context of the word in a multidimensional space instead of counting occurences. This way, the context became easier to grasp by NLP algorithms. The true extend of word embeddings comes in when transfer learning is utilized. Open-source pre-trained models allow developers to access to models that are trained on massive text corpusses. Therefore, in the Kaggle competitions and practical use cases word embeddings became widespread compared to BOW and TF-IDF to achieve higher accuracy. \n",
        "\n",
        "In Quora Question Pairs challenge, I wanted to utilize the same approach in different ways. I wanted to utilize sentence and word embeddings and compare their impact on the performance. Initially, I started working with Google's Universal Sentence Encoder and then, S-BERT sentence embeddings to classify questions as duplicate. Moreover, I have also implemented state of the art MaLSTM model for the classification purpose. After these implementations I wanted to make a custom model implementation for a siamese network. Using Bayesian Optimization I have created a vast parameter space for the algorithm. \n",
        "\n",
        "The custom model validation accuracy could not surpass the sentence embeddings performances. Considering the computationally heavy nature of the task and scarcity of time. It was a very hard task to overcome the performances of these state of the art models. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOY-uZ2li03s",
        "colab_type": "text"
      },
      "source": [
        "## Things To Do If Given More Time\n",
        "\n",
        "\n",
        "\n",
        "*   Train data on larger sets for MalSTM. Although it uses pre-trained embeddings, a larger dataset would improve the performance of LSTM layer because there would be more example for the LSTM to learn from. \n",
        "*   I would examine which pairs of questions were falsely labeled and depending on the analysis, if there is any systematic error, I would diagnose and come up with a solution. \n",
        "*   The Bayesian Optimization algorithms evaluation number was 5. It takes 5-6 hours to train 5 evaluations on GPU. The fact that 5 evaluations did not let Bayesian Optimization to learn from outcomes. Therefore, due to time constraints, that algorithm could not utilized 100%. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE8A3IOz8yLi",
        "colab_type": "text"
      },
      "source": [
        "## Citations \n",
        "\n",
        "\n",
        "*   Reimers, Nils, and Iryna Gurevych. “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.” Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, doi:10.18653/v1/d19-1410.\n",
        "\n",
        "*   Cer, Daniel & Yang, Yinfei & Kong, Sheng-yi & Hua, Nan & Limtiaco, Nicole & John, Rhomni & Constant, Noah & Guajardo-Cespedes, Mario & Yuan, Steve & Tar, Chris & Sung, Yun-Hsuan & Strope, Brian & Kurzweil, Ray. (2018). Universal Sentence Encoder. \n",
        "\n",
        "*   Neculoiu, Paul, et al. “Learning Text Similarity with Siamese Recurrent Networks.” Proceedings of the 1st Workshop on Representation Learning for NLP, 2016, doi:10.18653/v1/w16-1617.\n",
        "\n",
        "*   Bergstra, J., Yamins, D., Cox, D. D. (2013) Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures. To appear in Proc. of the 30th International Conference on Machine Learning (ICML 2013).\n",
        "\n",
        "*   Mansukhani, Subir. “HyperOpt: Bayesian Hyperparameter Optimization.” Data Science Blog by Domino, blog.dominodatalab.com/hyperopt-bayesian-hyperparameter-optimization/.\n",
        "\n",
        "*   Zelros. “From Bag of Words to Transformers: 10 Years of Practical Natural Language Processing.” Medium, Medium, 3 Nov. 2019, medium.com/@Zelros/from-bag-of-words-to-transformers-10-years-of-practical-natural-language-processing-8ccc238f679a.\n",
        "\n"
      ]
    }
  ]
}