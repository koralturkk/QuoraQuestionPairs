{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuoraQuestionPairs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpwHFX7ZhR/p/3jrnFAwMl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koralturkk/QuoraQuestionPairs/blob/master/QuoraQuestionPairs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klBJ3JqVS0oe",
        "colab_type": "text"
      },
      "source": [
        "# Quora Question Pairs\n",
        "\n",
        "Quora Question Pairs challenge is a semantic similarity problem. In the notebook, I have followed word and sentence embeddings to vectorize words and sentences to extract patterns and find out degrees of similarity between sentences.\n",
        "\n",
        "The following notebook is consisted of parts listed below to approach the problem. \n",
        "\n",
        "\n",
        "\n",
        "1.   Explanatory Data Analysis\n",
        "\n",
        "\n",
        "          *   Stratified Sampling\n",
        "          *   Train/Val/Test Split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2.   Models for Semantic Similarity\n",
        "\n",
        "\n",
        "          2.1. Sentence Embeddings\n",
        "\n",
        "              *   Google's Universal Sentence Encoder\n",
        "              *   Sentence-BERT \n",
        "\n",
        "\n",
        "          2.2. Word Embeddings\n",
        "\n",
        "              *   Google News's Word2Vec and Siamese Network\n",
        "\n",
        "3. Overview and Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MvFK2Z2URY4",
        "colab_type": "text"
      },
      "source": [
        "In the notebook, pre-trained embedding layers are used to vectorize words and sentences. Transfer Learning in NLP tasks are widely used approach when the computational and data resources are scarce. The embedding models are generated by training process of state of the art models on very large text corpus. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRp6V2Ql8-Ty",
        "colab_type": "text"
      },
      "source": [
        "#### Link to drive and import custom packages\n",
        "\n",
        "The "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHMNGzjj87pM",
        "colab_type": "code",
        "outputId": "06497e5b-5442-443f-97f9-7fd52c4c8774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append('/content/drive/My Drive/Colab Notebooks')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sDq2ewX9Iu6",
        "colab_type": "text"
      },
      "source": [
        "#### Loading Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7NY6_7ZoxQP",
        "colab_type": "code",
        "outputId": "29fce594-d4f5-4293-fd02-3d769a27a9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow_hub as hub\n",
        "import seaborn as sns\n",
        "import os, re, io, random\n",
        "from absl import logging\n",
        "from google.colab import files\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score \n",
        "from sentence_transformers import SentenceTransformer\n",
        "import scipy.spatial\n",
        "import json\n",
        "import torch ## loading file\n",
        "from google.colab import drive\n",
        "import nltk ## stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding,Activation,Flatten,merge,TimeDistributed,CuDNNGRU,Bidirectional\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import concatenate,subtract,add,maximum,multiply,Layer,Lambda\n",
        "from keras.backend import backend as K\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import KeyedVectors\n",
        "import itertools\n",
        "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
        "from keras import optimizers\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6bgHxBYVd3H",
        "colab_type": "text"
      },
      "source": [
        "\"nltk\" package will provide a list of stop words that will be useful when cleaning the text.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suk9bJ4U2FDh",
        "colab_type": "code",
        "outputId": "788f9a94-efab-41d7-d4cd-89917912aac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "stop_words = nltk.download('stopwords')\n",
        "stop_words = stopwords.words(\"english\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYgV0VpVazxE",
        "colab_type": "text"
      },
      "source": [
        "### Saving and Loading Files\n",
        "\n",
        "Due to the storage heavy nature of embeddings and models, it is a good practice to store processesed embeddings and models to avoid overhead. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A12F0H60aouJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_obj(obj, name):\n",
        "    with open(\"/content/drive/My Drive/\"+ name + \".pkl\", \"wb\") as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_obj(name):\n",
        "    with open(\"/content/drive/My Drive/\"+ name + \".pkl\", \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "#save = save_obj(embedding_dict, \"embeddings\")\n",
        "\n",
        "#embedding_dict = {}\n",
        "\n",
        "model_save_name = 'embeddings.pt'\n",
        "path = F\"/content/drive/My Drive/{model_save_name}\" \n",
        "\n",
        "def add_to_embedding_dict(key:str, value):\n",
        "  embedding_dict.update({key:value})\n",
        "\n",
        "## To save a file\n",
        "#torch.save(embedding_dict, path)\n",
        "#save_obj(embedding_dict,'embeddings')\n",
        "\n",
        "\n",
        "## To load a file \n",
        "#embedding_dict = torch.load(path)\n",
        "\n",
        "embedding_dict = load_obj(\"embeddings\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UWkxMSk-hwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv(\"/content/drive/My Drive/Datasets/train.csv\")\n",
        "#test_data = pd.read_csv(io.BytesIO(uploaded['test.csv']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAYyuerypY-3",
        "colab_type": "text"
      },
      "source": [
        "## Exploratory Data Analysis \n",
        "\n",
        "Make a statement about length of sentences and their contribution to performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9dZj_hepiXA",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing \n",
        "\n",
        "#### Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiRGz3hkrSey",
        "colab_type": "code",
        "outputId": "5a3dd556-7789-436a-ea9e-4097fa7ba41b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Missing Values in question1 column: {}\".format(train_data[\"question1\"].isnull().sum()))\n",
        "print(\"Missing Values in question2 column: {}\".format(train_data[\"question2\"].isnull().sum()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing Values in question1 column: 1\n",
            "Missing Values in question2 column: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ujPsl2rzJ1",
        "colab_type": "text"
      },
      "source": [
        "#### Removing Rows with Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3kx32YAryA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = train_data.dropna(how='any',axis=0) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSXRC4ENsCf8",
        "colab_type": "code",
        "outputId": "1b6c4beb-1698-4765-9bc1-c30d83447732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Missing Values in question1 column: {}\".format(train_data[\"question1\"].isnull().sum()))\n",
        "print(\"Missing Values in question2 column: {}\".format(train_data[\"question2\"].isnull().sum()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing Values in question1 column: 0\n",
            "Missing Values in question2 column: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAtqUxw3rVUe",
        "colab_type": "text"
      },
      "source": [
        "#### Cleaning Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQsyKU6ypolZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_columns(df, columns = [], clean_stop_words = False):\n",
        "\n",
        "  for col in columns:\n",
        "    df.loc[:,col] = df.loc[:,col].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "    df.loc[:,col] = df.loc[:,col].str.replace(r\"\\d+\", \"\")\n",
        "    df.loc[:,col] = df.loc[:,col].str.replace('[^\\w\\s]','')\n",
        "    df.loc[:,col] = df.loc[:,col].str.replace(r\"[︰-＠]\", \"\")\n",
        "\n",
        "    if clean_stop_words:\n",
        "      df.loc[:,col] = df.loc[:,col].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words)) ##stop words\n",
        "\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taj2V175TsGH",
        "colab_type": "code",
        "outputId": "3c0a4090-0cbc-4b52-ed68-153e8030b7fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(train_data.tail())\n",
        "print(train_data.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            id  ...  is_duplicate\n",
            "404285  404285  ...             0\n",
            "404286  404286  ...             1\n",
            "404287  404287  ...             0\n",
            "404288  404288  ...             0\n",
            "404289  404289  ...             0\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "   id  qid1  ...                                          question2 is_duplicate\n",
            "0   0     1  ...  What is the step by step guide to invest in sh...            0\n",
            "1   1     3  ...  What would happen if the Indian government sto...            0\n",
            "2   2     5  ...  How can Internet speed be increased by hacking...            0\n",
            "3   3     7  ...  Find the remainder when [math]23^{24}[/math] i...            0\n",
            "4   4     9  ...            Which fish would survive in salt water?            0\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSWfjGLmcPT9",
        "colab_type": "code",
        "outputId": "09f597b8-430d-4291-e339-5874b2216f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "train_data = clean_columns(train_data, [\"question1\", \"question2\"])\n",
        "print(train_data.tail())\n",
        "print(train_data.head())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            id  ...  is_duplicate\n",
            "404285  404285  ...             0\n",
            "404286  404286  ...             1\n",
            "404287  404287  ...             0\n",
            "404288  404288  ...             0\n",
            "404289  404289  ...             0\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "   id  qid1  ...                                          question2 is_duplicate\n",
            "0   0     1  ...  what is the step by step guide to invest in sh...            0\n",
            "1   1     3  ...  what would happen if the indian government sto...            0\n",
            "2   2     5  ...  how can internet speed be increased by hacking...            0\n",
            "3   3     7  ...    find the remainder when mathmath is divided by             0\n",
            "4   4     9  ...             which fish would survive in salt water            0\n",
            "\n",
            "[5 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihMx9H2wpxbv",
        "colab_type": "text"
      },
      "source": [
        "### Sampling Training Set\n",
        "\n",
        "Since the data set is too large. I will work on sample data to build NLP model to make to training and optimization process more robust. It is important to preserve the distribution to build a viable model for deployment.\n",
        "\n",
        "Therefore, I will used stratified sampling to sample data that has same distribution of \"is_duplicate\" labels from the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXknxZLlrif1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = train_data[\"is_duplicate\"]\n",
        "X = train_data.copy(deep=False).drop(columns = [\"is_duplicate\"])\n",
        "\n",
        "X_source, X_sample, y_source, y_sample = train_test_split(X, y, test_size=0.1, random_state=42, stratify= y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOm8GkierktI",
        "colab_type": "code",
        "outputId": "bd2dd3e2-ad83-4f98-cc44-5653b353ae37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y_sample.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    25503\n",
              "1    14926\n",
              "Name: is_duplicate, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fjNvJUwrnXA",
        "colab_type": "code",
        "outputId": "2b712362-b923-4c72-b790-dca50415caf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Sample distribution of label 1: \",y_sample.value_counts()[1]/y_sample.value_counts().sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample distribution of label 1:  0.369190432610255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldc0BMI2roP4",
        "colab_type": "code",
        "outputId": "c40e0818-51f8-4f86-eb7b-df2290f5b316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_data.is_duplicate.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    255024\n",
              "1    149263\n",
              "Name: is_duplicate, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UBPgpRVrrKO",
        "colab_type": "code",
        "outputId": "c1f136e1-244e-488b-936f-df3e4e5a76d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Source data distribution of label 1: \", train_data.is_duplicate.value_counts()[1]/train_data.is_duplicate.value_counts().sum())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source data distribution of label 1:  0.3692005926482919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqFLGd_NsIDq",
        "colab_type": "text"
      },
      "source": [
        "The distribution of labels are very close to each other. We can move forward with the implementation on sample data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLemNqxl_omN",
        "colab_type": "text"
      },
      "source": [
        "### Train/Val/Test Set\n",
        "\n",
        "Seperation of data into train/val/test set help us to validate the generalizability of models. \n",
        "\n",
        "The model is trained on training data, tuned on val data and final test is to check results on test data to see how valid the model is on unseen examples. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytzlsGWtAB7R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42, stratify = y_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dXb1BGjD0vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.reset_index(inplace=True)\n",
        "X_test.reset_index(inplace=True)\n",
        "#y_train.reset_index(inplace=True)\n",
        "#y_test.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73VoyF89r-aj",
        "colab_type": "text"
      },
      "source": [
        "# Models for Semantic Similarity\n",
        "\n",
        "Models for semantic similarity use embeddings to vectorize sentences or words. Then various distance metrics are calculated to measure variations between vector outputs. Some of the most commonly used distance metrics are Cosine Similarity, Manhattan Distance, Minkowski Distance and Euclidean Distance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_KflXhgqNbd",
        "colab_type": "text"
      },
      "source": [
        "#### Distance Metrics \n",
        "\n",
        "Distance metrics help us identify the relative difference or similarity between different vectors. Which distance metrics to be used relies on the nature of the problem. Moreover, distance metrics that are suitable for the problem scope can be accepted as a hyperparameter, therefore, they can be subjected comparison by the f1 score they yield. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wow17aQ-jHKX",
        "colab_type": "text"
      },
      "source": [
        "#### Useful Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfDVRP65hqSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report_predictions(y_train, predictions):\n",
        "  actual = y_train.to_list()\n",
        "  results = confusion_matrix(actual, predictions) \n",
        "  print('Confusion Matrix :')\n",
        "  print(results) \n",
        "  print('Accuracy Score :',accuracy_score(actual, predictions))\n",
        "  print('Report : ')\n",
        "  print(classification_report(actual, predictions))\n",
        "\n",
        "def cosine_similarities(embedding_1, embedding_2):\n",
        "  if len(embedding_1) == len(embedding_2):\n",
        "    length = len(embedding_1)\n",
        "  else:\n",
        "    return print(\"Array sizes do not match\")\n",
        "\n",
        "  similarities = []\n",
        "\n",
        "  for i in range(length):\n",
        "    sentence_1 = np.reshape(embedding_1[i], (1, -1))\n",
        "    sentence_2 = np.reshape(embedding_2[i], (1, -1))\n",
        "    similarity = float(cosine_similarity(sentence_1,sentence_2))\n",
        "    similarities.append(similarity)\n",
        "\n",
        "  return np.reshape(similarities, (-1,1))\n",
        "\n",
        "def get_predictions(similarities, threshold = 0.8):\n",
        "  predictions = list(map(float, similarities>threshold))\n",
        "  return predictions\n",
        "\n",
        "def find_max_word_count(df, columns):\n",
        "  count = 0\n",
        "  for col in columns:\n",
        "    new_count = max(df[col].str.split().map(len))\n",
        "    if new_count > count:\n",
        "      count = new_count\n",
        "\n",
        "    else:\n",
        "      continue\n",
        "  return count\n",
        "\n",
        "\n",
        "def tokenize(df, questions_cols):\n",
        " \n",
        "  for index, row in df.iterrows():\n",
        "\n",
        "      for question in questions_cols:\n",
        "\n",
        "          q2n = [] \n",
        "          for word in row[question].split():\n",
        "              if word in stop_words and word not in word2vec.vocab:\n",
        "                  continue\n",
        "\n",
        "              if word not in vocabulary:\n",
        "                  vocabulary[word] = len(inverse_vocabulary)\n",
        "                  q2n.append(len(inverse_vocabulary))\n",
        "                  inverse_vocabulary.append(word)\n",
        "              else:\n",
        "                  q2n.append(vocabulary[word])\n",
        "\n",
        "          df.set_value(index, question, q2n)\n",
        "\n",
        "  return df\n",
        "\n",
        "def build_embedding_matrix(vocabulary, embedding_dim =300):\n",
        "  embeddings = 1 * np.random.randn(len(vocabulary) + 1, embedding_dim)  # This will be the embedding matrix\n",
        "  embeddings[0] = 0  # So that the padding will be ignored\n",
        "\n",
        "# Build the embedding matrix\n",
        "  for word, index in vocabulary.items():\n",
        "    if word in word2vec.vocab:\n",
        "      embeddings[index] = word2vec.word_vec(word)\n",
        "\n",
        "  return embeddings "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9sM0aBngpOX",
        "colab_type": "text"
      },
      "source": [
        "## Sentence Embedding Models\n",
        "\n",
        "Sentence Embedding Models embeds full sentences into a vector representation. On this task I will implement and assess results of Universal Sentence Encoder and Sentence-Bert.\n",
        "\n",
        "\n",
        "*   Google's Universal Sentence Encoder\n",
        "\n",
        "        1.   Deep Averaging Network\n",
        "        2.   Transformer\n",
        "\n",
        "\n",
        "*   Sentence-Bert \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMwLlbYFsDwY",
        "colab_type": "text"
      },
      "source": [
        "Paper \n",
        "Take a look!!   \n",
        "-https://arxiv.org/pdf/1907.04307.pdf  \n",
        "-https://www.learnopencv.com/universal-sentence-encoder/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yePjxdLp76Yz",
        "colab_type": "text"
      },
      "source": [
        "### Universal Sentence Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEZX0aKO_dzJ",
        "colab_type": "text"
      },
      "source": [
        "There are 2 embeddings shared with the community in Universal Sentence Encoder. Both trained with different architectures, Deep Averaging Networks and Transformer Networks. These architectures present trade-offs, DAN model is computationally less expensive and has less accuracy overall while the model with transformer encoder scores higher accuracy with more computational costs. \n",
        "\n",
        "I will try to implement both for the project to see their impact on predictions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV8ewnAvAF8m",
        "colab_type": "text"
      },
      "source": [
        "#### Universal Sentence Encoder Trained with Deep Averaging Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ5qUC3J74Wk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DAN_module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\"]\n",
        "\n",
        "# Reduce logging output.\n",
        "#logging.set_verbosity(logging.ERROR)\n",
        "\n",
        "#DAN_module_embedding = hub.KerasLayer(DAN_module_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmMmDdmB-3KB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Create\n",
        "\n",
        "#question_1_list = X_train.loc[:,\"question1\"].to_list()\n",
        "#question_2_list = X_train.loc[:,\"question2\"].to_list()\n",
        "\n",
        "\n",
        "#with tf.Session() as session:\n",
        " # session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  #question_1_DAN_embeddings = session.run(DAN_module_embedding(question_1_list))\n",
        "  #question_2_DAN_embeddings = session.run(DAN_module_embedding(question_2_list))\n",
        "\n",
        "## Load\n",
        "question_1_DAN_embeddings, question_2_DAN_embeddings = embedding_dict[\"question_1_DAN_embeddings\"], embedding_dict[\"question_2_DAN_embeddings\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfweCVkX_TQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DAN_similarities = cosine_similarities(question_1_DAN_embeddings,question_2_DAN_embeddings)\n",
        "DAN_predictions = get_predictions(DAN_similarities, 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbRNRnbKU5OX",
        "colab_type": "code",
        "outputId": "039fd0cc-cba4-49ac-b78a-7b772c782507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "DAN_report = report_predictions(y_train, DAN_predictions)\n",
        "DAN_report"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix :\n",
            "[[15809  4593]\n",
            " [ 4214  7727]]\n",
            "Accuracy Score : 0.7276999659895496\n",
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.77      0.78     20402\n",
            "           1       0.63      0.65      0.64     11941\n",
            "\n",
            "    accuracy                           0.73     32343\n",
            "   macro avg       0.71      0.71      0.71     32343\n",
            "weighted avg       0.73      0.73      0.73     32343\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSw0620apHkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Save\n",
        "add_to_embedding_dict(\"question_1_DAN_embeddings\",question_1_DAN_embeddings)\n",
        "add_to_embedding_dict(\"question_2_DAN_embeddings\",question_2_DAN_embeddings)\n",
        "#torch.save(embedding_dict, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgFFGJZPCCTH",
        "colab_type": "text"
      },
      "source": [
        "#### Universal Sentence Encoder with Transformer Encoder\n",
        "\n",
        "Crashed due to RAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMgWjBbDCGOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TRANS_module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "#TRANS_module_embedding = hub.load(TRANS_module_url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUO2aBOCDmHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with tf.Session() as session:\n",
        " # session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "  #question_1_TRANS_embeddings, question_2_TRANS_embeddings = session.run([TRANS_module_embedding(question_1_list), TRANS_module_embedding(question_2_list)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whXW606lEAbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TRANS_similarities = cosine_similarities(question_1_TRANS_embeddings,question_2_TRANS_embeddings)\n",
        "#TRANS_predictions = get_predictions(similarities, 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucy1dUjaEL2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#report_predictions(y_train, TRANS_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cHec7GkWmb5",
        "colab_type": "text"
      },
      "source": [
        "### Sentence-BERT\n",
        "\n",
        "\n",
        "Researchers in Ubiquitous Knowledge Processing Labb (UKP-TUDA) implemented Sentence-Bert model which is a modification of a pretrained BERT network. The model uses siamese and triplet network to derive semantically meaningful sentence embeddings. This way, they have utilized BERT to be used for new tasks such as semantic similarity which was not possible before.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1N9WoMf0fIN8",
        "colab_type": "code",
        "outputId": "2ea19ade-48ba-4987-e237-d3450af8a83e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Encoding Text into Sentence Embeddings\n",
        "\n",
        "bert_nli_stsb_mean_transformer = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n",
        "bert_nli_stsb_mean_question_1_embeddings = bert_nli_stsb_mean_transformer.encode(question_1_list)\n",
        "bert_nli_stsb_mean_question_2_embeddings = bert_nli_stsb_mean_transformer.encode(question_2_list)\n",
        "\n",
        "### Loading pre-encoded sentence embeddings\n",
        "\n",
        "#bert_nli_stsb_mean_question_1_embeddings = embedding_dict[\"bert_nli_stsb_mean_question_1_embeddings\"]\n",
        "#bert_nli_stsb_mean_question_2_embeddings = embedding_dict[\"bert_nli_stsb_mean_question_2_embeddings\"]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:05<00:00, 71.9MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKg4FlK-XYID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## To save a file\n",
        "\n",
        "\n",
        "#add_to_embedding_dict(\"bert_nli_stsb_mean_question_1_embeddings\",bert_nli_stsb_mean_question_1_embeddings)\n",
        "#add_to_embedding_dict(\"bert_nli_stsb_mean_question_2_embeddings\", bert_nli_stsb_mean_question_2_embeddings)\n",
        "\n",
        "\n",
        "#save_obj(embedding_dict,\"embeddings\")\n",
        "\n",
        "#torch.save(embedding_dict, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN0Vu1F3-UMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sbert_similarities = cosine_similarities(bert_nli_stsb_mean_question_1_embeddings,bert_nli_stsb_mean_question_2_embeddings)\n",
        "sbert_predictions = get_predictions(sbert_similarities, 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKYm2tnp-Xz6",
        "colab_type": "code",
        "outputId": "3099e559-df05-4555-9b10-cce780171df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "sbert_report = report_predictions(y_train, sbert_predictions)\n",
        "sbert_report"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix :\n",
            "[[16234  4168]\n",
            " [ 4578  7363]]\n",
            "Accuracy Score : 0.7295860000618372\n",
            "Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.80      0.79     20402\n",
            "           1       0.64      0.62      0.63     11941\n",
            "\n",
            "    accuracy                           0.73     32343\n",
            "   macro avg       0.71      0.71      0.71     32343\n",
            "weighted avg       0.73      0.73      0.73     32343\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAt4djr34mKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#del bert_nli_stsb_mean_transformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P1avGVWjhe7",
        "colab_type": "text"
      },
      "source": [
        "## Word Embedding Models\n",
        "\n",
        "https://github.com/eliorc/Medium/blob/master/MaLSTM.ipynb\n",
        "\n",
        "In the word embedding models, in order to have achieve a \"is duplicate\" binary classification objective, we need to feed word embedding sequences to a siamese network. Then, the encoded sequence will decide how much semantically similar these two input sequences by calculating the cosine similarity between two outputs of the network. \n",
        "\n",
        "\n",
        "The choice of word embedding to vectorize words is important for performance. Therefore, I would choose to work with Google News' word2vec embeddings. \n",
        "\n",
        "The selection of word embeddings has to be done empirically. Each trial has to be documented and we should deploy model that yield highest accuracy and f1 score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03bKEV9_qsM7",
        "colab_type": "code",
        "outputId": "3f705c9b-5160-4b83-c580-dd7bd22493ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## max word count is needed to implement sequences. So that we can instantiate sequences that can encapsulate all the sentence in the dataset. \n",
        "max_word_count = find_max_word_count(train_data,[\"question1\", \"question2\"])\n",
        "print(max_word_count)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax4tfhrYL3tP",
        "colab_type": "text"
      },
      "source": [
        "#### Word2Vec\n",
        "\n",
        "After the creation of embedding matrix for the word that we have, it is not necessary to load the GoogleNews Embedding File due to its size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8xTDcx9L5U6",
        "colab_type": "code",
        "outputId": "3e9bc964-215e-45e0-f238-9818f8871c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#word2vec_embedding_file = \"/content/drive/My Drive/GoogleNews-vectors-negative300.bin.gz\"\n",
        "#word2vec = KeyedVectors.load_word2vec_format(word2vec_embedding_file, binary=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNvwehwGYbwv",
        "colab_type": "text"
      },
      "source": [
        "#### Word Embedding Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzTY54JZuzpZ",
        "colab_type": "code",
        "outputId": "43f7e1f8-ab8b-45dc-b7b2-7b3d9082c9af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "vocabulary = dict()\n",
        "inverse_vocabulary = ['<unk>']  # '<unk>' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
        "questions_cols = ['question1', 'question2']\n",
        "\n",
        "## Tokenize questions \n",
        "\n",
        "X_w2v = clean_columns(X_sample.copy(deep=False), columns = questions_cols, clean_stop_words = True)\n",
        "X_w2v = tokenize(X_w2v,questions_cols)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUetaY3ZMVzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Embedding \n",
        "word_2_vec_embeddings = embedding_dict[\"word_2_vec_embeddings\"]   #build_embedding_matrix(vocabulary,embedding_dim =300) \n",
        "\n",
        "## Delete \n",
        "#del word2vec\n",
        "\n",
        "#add_to_embedding_dict(\"word_2_vec_embeddings\",word_2_vec_embeddings)\n",
        "\n",
        "#save_obj(embedding_dict,\"embeddings\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIf5Mfx2NCa-",
        "colab_type": "code",
        "outputId": "488b83d7-9fd4-4a1a-cec4-bf5130a129c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "X_w2v.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>qid1</th>\n",
              "      <th>qid2</th>\n",
              "      <th>question1</th>\n",
              "      <th>question2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>156326</th>\n",
              "      <td>156326</td>\n",
              "      <td>244682</td>\n",
              "      <td>244683</td>\n",
              "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]</td>\n",
              "      <td>[12, 1, 2, 13, 9, 14, 15, 10, 16, 17, 18, 19]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233222</th>\n",
              "      <td>233222</td>\n",
              "      <td>343427</td>\n",
              "      <td>343428</td>\n",
              "      <td>[20, 21, 22, 23, 24]</td>\n",
              "      <td>[25, 21, 26, 27, 28]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318647</th>\n",
              "      <td>318647</td>\n",
              "      <td>444025</td>\n",
              "      <td>444026</td>\n",
              "      <td>[29, 30, 31]</td>\n",
              "      <td>[29, 30, 31]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401836</th>\n",
              "      <td>401836</td>\n",
              "      <td>535283</td>\n",
              "      <td>535284</td>\n",
              "      <td>[32, 33, 34]</td>\n",
              "      <td>[32, 33]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3053</th>\n",
              "      <td>3053</td>\n",
              "      <td>6053</td>\n",
              "      <td>6054</td>\n",
              "      <td>[35, 36]</td>\n",
              "      <td>[35, 36]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  ...                                      question2\n",
              "156326  156326  ...  [12, 1, 2, 13, 9, 14, 15, 10, 16, 17, 18, 19]\n",
              "233222  233222  ...                           [25, 21, 26, 27, 28]\n",
              "318647  318647  ...                                   [29, 30, 31]\n",
              "401836  401836  ...                                       [32, 33]\n",
              "3053      3053  ...                                       [35, 36]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBT_czm4enPV",
        "colab_type": "text"
      },
      "source": [
        "### Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLlTblATTeUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_w2v, X_val_w2v, y_train_w2v, y_val_w2v = train_test_split(X_w2v, y_sample, test_size=0.2, random_state=42, stratify = y_sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVH-EAOSbg3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Changing X_train_w2v to dict will provide greater ease at siamese network\n",
        "\n",
        "X_train_w2v = {'left': X_train_w2v.question1, 'right': X_train_w2v.question2}\n",
        "X_val_w2v = {'left': X_val_w2v.question1, 'right': X_val_w2v.question2}\n",
        "\n",
        "# Convert labels to their numpy representations\n",
        "y_train_w2v = y_train_w2v.values\n",
        "y_val_w2v = y_val_w2v.values\n",
        "\n",
        "# Zero padding\n",
        "for dataset, side in itertools.product([X_train_w2v, X_val_w2v], ['left', 'right']):\n",
        "    dataset[side] = pad_sequences(dataset[side], maxlen=max_word_count)\n",
        "\n",
        "# Make sure everything is ok\n",
        "assert X_train_w2v['left'].shape == X_train_w2v['right'].shape\n",
        "assert len(X_train_w2v['left']) == len(y_train_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjB9zoT_eXpN",
        "colab_type": "code",
        "outputId": "d417d0d1-c86e-451b-b240-1c1071969f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "### Model Implementation\n",
        "\n",
        "# Model variables\n",
        "embedding_dim = 300\n",
        "n_hidden = 50\n",
        "gradient_clipping_norm = 1.25\n",
        "batch_size = 64\n",
        "n_epoch = 25\n",
        "\n",
        "from keras.layers import Input, Embedding, LSTM, Lambda, Bidirectional\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adadelta, SGD\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "import time\n",
        "\n",
        "def cosine_distance(x1, x2):\n",
        "    x1 = K.l2_normalize(x1, axis=-1)\n",
        "    x2 = K.l2_normalize(x2, axis=-1)\n",
        "    return -K.mean(x1 * x2, axis=-1, keepdims=True)\n",
        "\n",
        "def cos_dist_output_shape(shape_1, shape_2):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0],1)\n",
        "\n",
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
        "\n",
        "\n",
        "# The visible layer\n",
        "left_input = Input(shape=(max_word_count,), dtype='int32')\n",
        "right_input = Input(shape=(max_word_count,), dtype='int32')\n",
        "\n",
        "embedding_layer = Embedding(len(word_2_vec_embeddings), embedding_dim, weights=[word_2_vec_embeddings], input_length=max_word_count, trainable=False)\n",
        "\n",
        "# Embedded version of the inputs\n",
        "encoded_left = embedding_layer(left_input)\n",
        "encoded_right = embedding_layer(right_input)\n",
        "\n",
        "# Since this is a siamese network, both sides share the same LSTM\n",
        "shared_lstm = Bidirectional(LSTM(n_hidden))\n",
        "\n",
        "left_output = shared_lstm(encoded_left)\n",
        "right_output = shared_lstm(encoded_right)\n",
        "\n",
        "\n",
        "# Calculates the distance as defined by the MaLSTM model\n",
        "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "# Pack it all up into a model\n",
        "malstm = Model([left_input, right_input], [malstm_distance])\n",
        "\n",
        "# Adadelta optimizer, with gradient clipping by norm\n",
        "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "\n",
        "\n",
        "## Callbacks\n",
        "\n",
        "## Learning Rate \n",
        "\n",
        "#lr_schedule = LearningRateScheduler(lambda epoch: 1e-8*10**(epoch/5))\n",
        "\n",
        "## Checkpoint Callback\n",
        "filepath=\"/content/drive/My Drive/weights.best.hdf5\"\n",
        "model_checkpoint = ModelCheckpoint(filepath, monitor=\"val_acc\", verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [model_checkpoint]\n",
        "\n",
        "# load weights\n",
        "#malstm = malstm.load_weights(\"/content/drive/My Drive/weights.best.hdf5\")\n",
        "\n",
        "malstm.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "\n",
        "# Start training\n",
        "\n",
        "malstm_trained = malstm.fit([X_train_w2v['left'], X_train_w2v['right']], y_train_w2v, batch_size=batch_size, epochs=n_epoch,\n",
        "                            validation_data=([X_val_w2v['left'], X_val_w2v['right']], y_val_w2v),callbacks=callbacks_list, verbose = 1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32343 samples, validate on 8086 samples\n",
            "Epoch 1/25\n",
            "32343/32343 [==============================] - 766s 24ms/step - loss: 0.2102 - acc: 0.6919 - val_loss: 0.1929 - val_acc: 0.7216\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.72162, saving model to /content/drive/My Drive/weights.best.hdf5\n",
            "Epoch 2/25\n",
            "13120/32343 [===========>..................] - ETA: 6:51 - loss: 0.1899 - acc: 0.7245"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcKumaSYjsCy",
        "colab_type": "code",
        "outputId": "7dd9d1d7-b7ce-4d3f-8a61-910841970490",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "learning_rate_plot = plt.semilogx(malstm_trained.history[\"lr\"], malstm_trained.history[\"loss\"])\n",
        "plt.axis([1e-8, 1e-4, 0, 60])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1e-08, 0.0001, 0, 60]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKyEr9inCrPx",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter optimization\n",
        "\n",
        "https://public.ukp.informatik.tu-darmstadt.de/reimers/Optimal_Hyperparameters_for_Deep_LSTM-Networks.pdf\n",
        "\n",
        "https://towardsdatascience.com/finding-the-right-architecture-for-neural-network-b0439efa4587\n",
        "\n",
        "Hyperparameters\n",
        "\n",
        "Recurrent Unit = [25,50,75,100]\n",
        "Batch_Size =  1, 8, 16, 32, and 64\n",
        "Learning Rate \n",
        "Distance Function \n",
        "\n",
        "---- Optimizer ---  \n",
        "Adam and Adam with Nesterov momentum\n",
        "(Nadam) usually performed the best, followed\n",
        "by RMSProp. Adadelta and Adagrad had a\n",
        "much higher variance in terms of test performance and resulted on average to far worse\n",
        "results. SGD failed in a high number of cases\n",
        "to converge to a minimum, likely due to its\n",
        "high sensitivity of the learning rate. Nadam\n",
        "was the fastest optimizer. \n",
        "\n",
        "\n",
        "Model Implementation => (Article) Siamese Recurrent Architectures for Learning Sentence Similarity\n",
        "\n",
        "\n",
        "Distance Metrics \n",
        "  \n",
        "-https://towardsdatascience.com/importance-of-distance-metrics-in-machine-learning-modelling-e51395ffe60d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2MHebmqOZq8",
        "colab_type": "code",
        "outputId": "de05c22f-a141-4079-8549-6de5fe304c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "source": [
        "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
        "from keras import optimizers\n",
        "\n",
        "search_space = {\n",
        "    'batch_size': hp.choice('bs', [8,16,32,64,128]),\n",
        "    'lstm1_nodes': hp.choice('units_lsmt1', [25,50,75,100]),\n",
        "    \"lr\": hp.uniform('lr',0,1),\n",
        "    \"epochs\": hp.choice('epochs', [30, 40, 50, 60, 70]),\n",
        "    \"optimizer\": hp.choice('optmz',[\"sgd\", \"rms\",\"adamdelta\"]),\n",
        "    \"distance_metric\": hp.choice('dist',[\"exponent_neg_manhattan_distance\", \"cosine_distance\"]),\n",
        "    \"loss\": hp.choice('loss',[\"huber_loss\", \"mean_squeared_error\"]),\n",
        "}\n",
        "\n",
        "\n",
        "def build_siamese_network(params):\n",
        "\n",
        "  \n",
        "  left_input = Input(shape=(max_word_count,), dtype='int32')\n",
        "  right_input = Input(shape=(max_word_count,), dtype='int32')\n",
        "\n",
        "  embedding_layer = Embedding(len(word_2_vec_embeddings), embedding_dim, weights=[word_2_vec_embeddings], input_length=max_word_count, trainable=False)\n",
        "\n",
        "  encoded_left = embedding_layer(left_input)\n",
        "  encoded_right = embedding_layer(right_input)\n",
        "\n",
        "  shared_lstm = Bidirectional(LSTM(params[\"lstm1_nodes\"]))   ##Bidirectional(LSTM(n_hidden))\n",
        "\n",
        "  left_output = shared_lstm(encoded_left)\n",
        "  right_output = shared_lstm(encoded_right)\n",
        "\n",
        "\n",
        "  if params[\"distance_metric\"] == \"exponent_neg_manhattan_distance\":\n",
        "    distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "  if params[\"distance_metric\"] == \"cosine_distance\":\n",
        "    distance = Lambda(function=lambda x: cosine_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
        "\n",
        "  model = Model([left_input, right_input], [distance])\n",
        "\n",
        "  ### Calbacks \n",
        "  file_name = 'siamese_callbacks.csv'\n",
        "  callback_file = F\"/content/drive/My Drive/{file_name}\" \n",
        "  csv_callback = keras.callbacks.CSVLogger(callback_file, separator=',', append=False)\n",
        "\n",
        "\n",
        "  lr = params[\"lr\"]\n",
        "  epochs = params[\"epochs\"]\n",
        "  batch_size = params[\"batch_size\"]\n",
        "\n",
        "  if params[\"optimizer\"] == 'rms':\n",
        "      optimizer = optimizers.RMSprop(lr=lr)\n",
        "  elif params[\"optimizer\"] == \"adadelta\":\n",
        "      optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "  else:\n",
        "      optimizer = optimizers.SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "\n",
        "  model.compile(loss=params[\"loss\"], optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  history = model.fit([X_train_w2v['left'], X_train_w2v['right']], y_train_w2v, batch_size=batch_size, epochs=epochs,\n",
        "                            validation_data=([X_val_w2v['left'], X_val_w2v['right']], y_val_w2v),verbose=2,callbacks=[csv_callback])\n",
        "  \n",
        "  val_error = np.amin(model.history['val_loss']) \n",
        "\n",
        "  print('Best validation error of epoch:', val_error)\n",
        "  return {'loss': val_error, 'status': STATUS_OK, 'model': lstm_model}\n",
        "\n",
        "trials = Trials()\n",
        "best = fmin(build_siamese_network,\n",
        "    space=search_space,\n",
        "    algo=tpe.suggest, # type random.suggest to select param values randomly\n",
        "    max_evals=200, # max number of evaluations you want to do on objective function\n",
        "    trials=trials)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 32343 samples, validate on 8086 samples\n",
            "Epoch 1/40\n",
            "  0%|          | 0/200 [00:07<?, ?it/s, best loss: ?]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-d524dc59e4c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# type random.suggest to select param values randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# max number of evaluations you want to do on objective function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     trials=trials)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-d524dc59e4c4>\u001b[0m in \u001b[0;36mbuild_siamese_network\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   history = model.fit([X_train_w2v['left'], X_train_w2v['right']], y_train_w2v, batch_size=batch_size, epochs=epochs,\n\u001b[0;32m---> 59\u001b[0;31m                             validation_data=([X_val_w2v['left'], X_val_w2v['right']], y_val_w2v),verbose=2,callbacks=[csv_callback])\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0mval_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE8A3IOz8yLi",
        "colab_type": "text"
      },
      "source": [
        "## Citations \n",
        "\n",
        "\n",
        "*   Reimers, Nils, and Iryna Gurevych. “Sentence-BERT: Sentence Embeddings Using Siamese BERT-Networks.” Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019, doi:10.18653/v1/d19-1410.\n",
        "\n",
        "*   Cer, Daniel & Yang, Yinfei & Kong, Sheng-yi & Hua, Nan & Limtiaco, Nicole & John, Rhomni & Constant, Noah & Guajardo-Cespedes, Mario & Yuan, Steve & Tar, Chris & Sung, Yun-Hsuan & Strope, Brian & Kurzweil, Ray. (2018). Universal Sentence Encoder. \n",
        "\n",
        "*   Neculoiu, Paul, et al. “Learning Text Similarity with Siamese Recurrent Networks.” Proceedings of the 1st Workshop on Representation Learning for NLP, 2016, doi:10.18653/v1/w16-1617.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpxGdE8AS9cT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}